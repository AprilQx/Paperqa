{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ldp.agent import SimpleAgent\n",
    "import nest_asyncio\n",
    "from aviary.core import Message, Tool,FunctionInfo\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input/output types for the calculator\n",
    "class CalculatorInput(BaseModel):\n",
    "    x: float = Field(description=\"First number\")\n",
    "    y: float = Field(description=\"Second number\")\n",
    "\n",
    "class CalculatorOutput(BaseModel):\n",
    "    result: float = Field(description=\"The sum of x and y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_simple_agent():\n",
    "    # Define the calculator function\n",
    "    async def calculator(input_data: CalculatorInput) -> CalculatorOutput:\n",
    "        return CalculatorOutput(result=input_data.x + input_data.y)\n",
    "\n",
    "    # Create the tool properly structured\n",
    "    function_info = FunctionInfo(\n",
    "        name=\"add\",\n",
    "        description=\"Add two numbers together\",\n",
    "        parameters=CalculatorInput.model_json_schema(),\n",
    "        function=calculator,\n",
    "        input_type=CalculatorInput,\n",
    "        output_type=CalculatorOutput\n",
    "    )\n",
    "\n",
    "    calculator_tool = Tool(\n",
    "        name=\"calculator\",\n",
    "        description=\"A simple calculator that can add two numbers\",\n",
    "        function=function_info\n",
    "    )\n",
    "\n",
    "    # Initialize the agent with the tool\n",
    "    agent = SimpleAgent(\n",
    "        llm_model={\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"temperature\": 0.1\n",
    "        },\n",
    "        sys_prompt=\"You are a helpful assistant that can perform calculations.\"\n",
    "    )\n",
    "\n",
    "    # Initialize agent state with tools\n",
    "    state = await agent.init_state(tools=[calculator_tool])\n",
    "\n",
    "    # Create a test message\n",
    "    test_message = Message(\n",
    "        role=\"user\",\n",
    "        content=\"Can you help me add 5 and 3?\"\n",
    "    )\n",
    "\n",
    "    # Get agent's action and new state\n",
    "    action, new_state, cost = await agent.get_asv(state, [test_message])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Action:\", action.value.content)\n",
    "    print(\"Cost:\", cost)\n",
    "    print(\"Total tokens used:\", new_state.total_tokens)\n",
    "    print(\"Total cost so far:\", new_state.total_cost)\n",
    "\n",
    "    return action, new_state, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/envs/paperqa_env/lib/python3.11/ast.py:50: RuntimeWarning: coroutine 'test_simple_agent' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: OpenAIException - Error code: 403 - {'error': {'message': 'Project `proj_8WO6cHG6Ics15ycd35qOE2RY` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/llms/openai/openai.py:770\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream)\u001b[0m\n\u001b[1;32m    757\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    759\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_aclient\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m     },\n\u001b[1;32m    768\u001b[0m )\n\u001b[0;32m--> 770\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_openai_chat_completion_request(\n\u001b[1;32m    771\u001b[0m     openai_aclient\u001b[38;5;241m=\u001b[39mopenai_aclient,\n\u001b[1;32m    772\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    773\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    774\u001b[0m     logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[1;32m    775\u001b[0m )\n\u001b[1;32m    776\u001b[0m stringified_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:131\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/llms/openai/openai.py:418\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/llms/openai/openai.py:400\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    402\u001b[0m         )\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/openai/_legacy_response.py:381\u001b[0m, in \u001b[0;36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/openai/resources/chat/completions.py:1720\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1719\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1722\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1723\u001b[0m         {\n\u001b[1;32m   1724\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1725\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1727\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1728\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1731\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1733\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1734\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1736\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1737\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1738\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1739\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1740\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1741\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1742\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1743\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1744\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1745\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1746\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1747\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1750\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1751\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1754\u001b[0m         },\n\u001b[1;32m   1755\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1756\u001b[0m     ),\n\u001b[1;32m   1757\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1758\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1759\u001b[0m     ),\n\u001b[1;32m   1760\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1761\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1762\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1763\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1846\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m )\n\u001b[0;32m-> 1849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/openai/_base_client.py:1644\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1643\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1647\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1648\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1653\u001b[0m )\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'message': 'Project `proj_8WO6cHG6Ics15ycd35qOE2RY` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/main.py:457\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, base_url, api_version, api_key, model_list, extra_headers, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutine(init_response):\n\u001b[0;32m--> 457\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/llms/openai/openai.py:816\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream)\u001b[0m\n\u001b[1;32m    814\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 816\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    817\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39mmessage, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[1;32m    818\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 403 - {'error': {'message': 'Project `proj_8WO6cHG6Ics15ycd35qOE2RY` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For Jupyter environment:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m test_simple_agent()\n",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m, in \u001b[0;36mtest_simple_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m test_message \u001b[38;5;241m=\u001b[39m Message(\n\u001b[1;32m     36\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you help me add 5 and 3?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Get agent's action and new state\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m action, new_state, cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mget_asv(state, [test_message])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/contextlib.py:97\u001b[0m, in \u001b[0;36mAsyncContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/GitLab_Projects/master_project/xx823/ldp/ldp/agent/simple_agent.py:207\u001b[0m, in \u001b[0;36mSimpleAgent.get_asv\u001b[0;34m(self, agent_state, obs)\u001b[0m\n\u001b[1;32m    199\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cost_calculator\u001b[38;5;241m.\u001b[39mnum_tokens_from_messages(\n\u001b[1;32m    200\u001b[0m     messages, \n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m )\n\u001b[1;32m    203\u001b[0m next_state\u001b[38;5;241m.\u001b[39mtotal_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_tokens\n\u001b[1;32m    205\u001b[0m result \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    206\u001b[0m     OpResult[ToolRequestMessage],\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_call_op(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_op(), msgs\u001b[38;5;241m=\u001b[39mmessages, tools\u001b[38;5;241m=\u001b[39mnext_state\u001b[38;5;241m.\u001b[39mtools\n\u001b[1;32m    209\u001b[0m     ),\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Calculate output tokens\u001b[39;00m\n\u001b[1;32m    213\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cost_calculator\u001b[38;5;241m.\u001b[39mnum_tokens_from_messages(\n\u001b[1;32m    214\u001b[0m     [result\u001b[38;5;241m.\u001b[39mvalue], \n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/contextlib.py:97\u001b[0m, in \u001b[0;36mAsyncContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/contextlib.py:97\u001b[0m, in \u001b[0;36mAsyncContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/GitLab_Projects/master_project/xx823/ldp/ldp/graph/ops.py:561\u001b[0m, in \u001b[0;36mOp.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mupdate(call_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, (var_args, kwargs))\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# actually call forward pass with unpacked args and kwargs\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39munpacked_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_kwargs)\n\u001b[1;32m    562\u001b[0m t_output: \u001b[38;5;28mtype\u001b[39m[TOutput_co] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(result)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Now package up my result so it can be consumed by other calls.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# Explicitly specify t_output. OpResult[TOutput] returns a generic object\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitLab_Projects/master_project/xx823/ldp/ldp/graph/common_ops.py:279\u001b[0m, in \u001b[0;36mLLMCallOp.forward\u001b[0;34m(self, config, msgs, tools, tool_choice)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tools:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# if no tools are provided, tool_choice must be 'none'\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 279\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_single_and_maybe_validate(\n\u001b[1;32m    280\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    281\u001b[0m     num_retries\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    282\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmsgs,\n\u001b[1;32m    283\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    284\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo messages returned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitLab_Projects/master_project/xx823/ldp/ldp/graph/common_ops.py:319\u001b[0m, in \u001b[0;36mLLMCallOp._call_single_and_maybe_validate\u001b[0;34m(self, model, num_retries, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_single_and_maybe_validate\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: LLMModel, num_retries: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_validator:\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# If a response validator is not supplied, then we should not do any retries here - leave\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# that for LiteLLM to handle.\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcall_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# NOTE: `num_retries` also gets passed to LiteLLM, so there could a maximum of\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# `num_retries**2` retries. TODO: consider if we should have separate parameters\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# for LiteLLM and validation retries.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;129m@tenacity\u001b[39m\u001b[38;5;241m.\u001b[39mretry(\n\u001b[1;32m    325\u001b[0m         retry\u001b[38;5;241m=\u001b[39mtenacity\u001b[38;5;241m.\u001b[39mretry_if_exception_type(ResponseValidationError),\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;66;03m# num_retries+1 because the first call is not a retry\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_and_validate\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/llmclient/llms.py:980\u001b[0m, in \u001b[0;36mMultipleCompletionLLMModel.call_single\u001b[0;34m(self, messages, callbacks, output_type, tools, tool_choice, **chat_kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_single\u001b[39m(\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    972\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[Message],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_kwargs,\n\u001b[1;32m    978\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 980\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m    981\u001b[0m             messages, callbacks, output_type, tools, tool_choice, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_kwargs\n\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/llmclient/llms.py:857\u001b[0m, in \u001b[0;36mMultipleCompletionLLMModel.call\u001b[0;34m(self, messages, callbacks, output_type, tools, tool_choice, **chat_kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[LLMResult] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 857\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39machat(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_kwargs)\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         validate_json_completion(completion, output_type)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/llmclient/llms.py:727\u001b[0m, in \u001b[0;36mMultipleCompletionLLMModel.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21machat\u001b[39m(\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages: Iterable[Message], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    726\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m litellm\u001b[38;5;241m.\u001b[39mModelResponse:\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m track_costs(litellm\u001b[38;5;241m.\u001b[39macompletion)(\n\u001b[1;32m    728\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[m\u001b[38;5;241m.\u001b[39mmodel_dump(by_alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m|\u001b[39m kwargs),\n\u001b[1;32m    730\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/llmclient/cost_tracker.py:88\u001b[0m, in \u001b[0;36mtrack_costs.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 88\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m GLOBAL_COST_TRACKER\u001b[38;5;241m.\u001b[39menabled\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m     90\u001b[0m         GLOBAL_COST_TRACKER\u001b[38;5;241m.\u001b[39mrecord(response)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/utils.py:1242\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\n\u001b[1;32m   1240\u001b[0m     e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_retries\n\u001b[1;32m   1241\u001b[0m )  \u001b[38;5;66;03m## IMPORTANT: returns the deployment's num_retries to the router\u001b[39;00m\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/utils.py:1107\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _caching_handler_response\u001b[38;5;241m.\u001b[39mfinal_embedding_cached_response\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1108\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/main.py:476\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, base_url, api_version, api_key, model_list, extra_headers, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    475\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2190\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2189\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:293\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid_request_error\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_not_found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m    291\u001b[0m ):\n\u001b[1;32m    292\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[1;32m    294\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    295\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    296\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    297\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    298\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA timeout occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n\u001b[1;32m    301\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: litellm.NotFoundError: OpenAIException - Error code: 403 - {'error': {'message': 'Project `proj_8WO6cHG6Ics15ycd35qOE2RY` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# For Jupyter environment:\n",
    "await test_simple_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

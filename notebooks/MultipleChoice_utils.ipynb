{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import string\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFUSE_CHOICE = \"Insufficient information to answer the question\"\n",
    "ALPHABET = string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultipleChoiceQuestion():\n",
    "    question: str\n",
    "    choices: list[str]         \n",
    "    correct_answer: str         \n",
    "    unsure_option: str          \n",
    "    sources: list[str]         \n",
    "    ideal: str                  \n",
    "    distractors: list[str]      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_choices(ideal: str, distractors: list[str]) -> tuple[list[str], str, str]:\n",
    "    # Combines correct answer, \"Insufficient information\" option, and distractors\n",
    "    choices = [ideal, REFUSE_CHOICE, *distractors]\n",
    "    \n",
    "    # Creates letter choices (A, B, C, etc)\n",
    "    n_choices = len(choices)\n",
    "    if n_choices > len(ALPHABET):\n",
    "        raise ValueError(\"Too many choices\")\n",
    "\n",
    "    # Randomizes the order\n",
    "    perm = list(range(n_choices))\n",
    "    random.shuffle(perm)\n",
    "    shuffled_choices = [\n",
    "        f\"({letter}) {choices[sigma_i]}\"\n",
    "        for letter, sigma_i in zip(ALPHABET, perm, strict=False)\n",
    "    ]\n",
    "\n",
    "    # Returns the correct answer letter and \"unsure\" option letter\n",
    "    answer = ALPHABET[perm.index(0)]\n",
    "    unsure = ALPHABET[perm.index(1)]\n",
    "\n",
    "    return shuffled_choices, answer, unsure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questions(questions: List[MultipleChoiceQuestion], output_dir: Path):\n",
    "    \"\"\"Save questions in multiple formats.\"\"\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(output_dir / \"questions.json\", 'w') as f:\n",
    "        json.dump([asdict(q) for q in questions], f, indent=2)\n",
    "    \n",
    "    # Save as JSONL (one question per line)\n",
    "    with open(output_dir / \"questions.jsonl\", 'w') as f:\n",
    "        for q in questions:\n",
    "            f.write(json.dumps(asdict(q)) + '\\n')\n",
    "    \n",
    "    # Save as CSV\n",
    "    with open(output_dir / \"questions.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header\n",
    "        headers = [\"question\", \"choices\", \"correct_answer\", \"unsure_option\", \"sources\", \"ideal\", \"distractors\"]\n",
    "        writer.writerow(headers)\n",
    "        # Write data\n",
    "        for q in questions:\n",
    "            writer.writerow([\n",
    "                q.question,\n",
    "                \"|\".join(q.choices),  # Join choices with pipe separator\n",
    "                q.correct_answer,\n",
    "                q.unsure_option,\n",
    "                \"|\".join(q.sources),\n",
    "                q.ideal,\n",
    "                \"|\".join(q.distractors)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_questions(save_dir: Path | str = \"formatted_questions_test\"):\n",
    "    \"\"\"\n",
    "    Format training questions from the LitQA2 dataset.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory to save the formatted questions\n",
    "    \n",
    "    Returns:\n",
    "        List of formatted MultipleChoiceQuestion objects\n",
    "    \"\"\"\n",
    "    # Load training dataset\n",
    "    print(\"Loading training dataset...\")\n",
    "    try:\n",
    "        # Login to Hugging Face\n",
    "        login(token=\"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\")\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"futurehouse/aviary-paper-data\", \"LitQA2\")\n",
    "        \n",
    "        # Try to access 'test' split, or fallback to whatever is available\n",
    "        if 'test' in dataset:\n",
    "            train_data = dataset['test']\n",
    "        else:\n",
    "            # Get the first available split\n",
    "            first_split = list(dataset.keys())[0]\n",
    "            train_data = dataset[first_split]\n",
    "            print(f\"'test' split not found, using '{first_split}' instead\")\n",
    "        \n",
    "        print(\"Loaded successfully\")\n",
    "        \n",
    "        # Check and print first item structure to help debug\n",
    "        if len(train_data) > 0:\n",
    "            first_item = train_data[0]\n",
    "            print(f\"First item keys: {list(first_item.keys())}\")\n",
    "            print(f\"Example question: {first_item.get('question', 'N/A')}\")\n",
    "        \n",
    "        # Format all questions\n",
    "        formatted_questions = []\n",
    "        for q in train_data:\n",
    "            choices, answer, unsure = randomize_choices(\n",
    "                ideal=q['ideal'],\n",
    "                distractors=q['distractors']\n",
    "            )\n",
    "            \n",
    "            formatted_questions.append(MultipleChoiceQuestion(\n",
    "                question=q['question'],\n",
    "                choices=choices,\n",
    "                correct_answer=answer,\n",
    "                unsure_option=unsure,\n",
    "                sources=q['sources'] if 'sources' in q else [],\n",
    "                ideal=q['ideal'],\n",
    "                distractors=q['distractors']\n",
    "            ))\n",
    "        \n",
    "        print(f\"\\nFormatted {len(formatted_questions)} questions\")\n",
    "        \n",
    "        # Save questions\n",
    "        output_dir = Path(save_dir)\n",
    "        save_questions(formatted_questions, output_dir)\n",
    "        print(f\"\\nSaved questions to {output_dir}\")\n",
    "        \n",
    "        # Print example\n",
    "        if formatted_questions:\n",
    "            print(\"\\nExample formatted question:\")\n",
    "            example = formatted_questions[0]\n",
    "            print(f\"Question: {example.question}\")\n",
    "            print(\"\\nChoices:\")\n",
    "            for choice in example.choices:\n",
    "                print(choice)\n",
    "            print(f\"\\nCorrect Answer: {example.correct_answer}\")\n",
    "            print(f\"'Unsure' Option: {example.unsure_option}\")\n",
    "            print(f\"Sources: {example.sources}\")\n",
    "        \n",
    "        return formatted_questions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting questions: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "Loaded successfully\n",
      "First item keys: ['id', 'question', 'ideal', 'distractors', 'canary', 'tag', 'version', 'sources', 'is_opensource', 'subtask', 'key-passage']\n",
      "Example question: Approximately what percentage of topologically associated domains in the GM12878 blood cell line does DiffDomain classify as reorganized in the K562 cell line?\n",
      "\n",
      "Formatted 49 questions\n",
      "\n",
      "Saved questions to formatted_questions_test\n",
      "\n",
      "Example formatted question:\n",
      "Question: Approximately what percentage of topologically associated domains in the GM12878 blood cell line does DiffDomain classify as reorganized in the K562 cell line?\n",
      "\n",
      "Choices:\n",
      "(A) 41%\n",
      "(B) 51%\n",
      "(C) 31%\n",
      "(D) Insufficient information to answer the question\n",
      "(E) 11%\n",
      "(F) 21%\n",
      "\n",
      "Correct Answer: C\n",
      "'Unsure' Option: D\n",
      "Sources: ['https://doi.org/10.1038/s41467-024-44782-6']\n"
     ]
    }
   ],
   "source": [
    "questions = format_training_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultipleChoiceQuestion(question='DK015 and DK038 strains of Verticillium dahliae have in common approximately what percentage orthologous genes?', choices=['(A) 97%', '(B) Insufficient information to answer the question', '(C) 95%', '(D) 98%', '(E) 96%', '(F) 94%'], correct_answer='C', unsure_option='B', sources=['https://doi.org/10.1186/s12915-024-01900-6'], ideal='95%', distractors=['94%', '96%', '97%', '98%'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

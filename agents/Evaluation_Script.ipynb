{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/envs/paperqa_env/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Mapping\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "from aviary.env import TaskDataset\n",
    "from ldp.agent import SimpleAgent\n",
    "from ldp.alg.callbacks import MeanMetricsCallback\n",
    "from ldp.alg.runners import Evaluator, EvaluatorConfig\n",
    "from paperqa import Settings\n",
    "from paperqa.agents.task import TASK_DATASET_NAME, LitQAv2TaskSplit\n",
    "from paperqa.settings import AgentSettings, IndexSettings\n",
    "from paperqa.litqa import (\n",
    "    read_litqa_v2_from_hub,\n",
    "    DEFAULT_LABBENCH_HF_HUB_NAME,\n",
    "    DEFAULT_AVIARY_PAPER_HF_HUB_NAME,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldp.agent.simple_agent import SimpleAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ldp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mldp\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'ldp' is not defined"
     ]
    }
   ],
   "source": [
    "import ldp\n",
    "print(ldp.__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aviary.env import TaskDataset\n",
    "from paperqa import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(paper_directory='/Users/apple/Documents/GitLab_Projects/master_project/xx823/papers')\n",
    "dataset = TaskDataset.from_name(TASK_DATASET_NAME, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from paperqa import Settings, Docs\n",
    "from paperqa.agents.main import agent_query\n",
    "from paperqa.agents.search import get_directory_index\n",
    "\n",
    "from aviary.env import TaskDataset\n",
    "from ldp.agent import SimpleAgent\n",
    "from ldp.alg.callbacks import MeanMetricsCallback\n",
    "from ldp.alg.runners import Evaluator, EvaluatorConfig\n",
    "from paperqa.agents.task import TASK_DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def amain(paper_dir: str | os.PathLike) -> None:\n",
    "    # Create settings with explicit configuration\n",
    "    settings = Settings(\n",
    "        paper_directory=paper_dir,\n",
    "        agent={\"index\": {\n",
    "            \"sync_with_paper_directory\": True,\n",
    "            \"recurse_subdirectories\": True\n",
    "        }}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        built_index = await get_directory_index(settings=settings)\n",
    "        \n",
    "        # Print index information \n",
    "        print(f\"Using index: {settings.get_index_name()}\")\n",
    "        index_files = await built_index.index_files\n",
    "        print(f\"Number of indexed files: {len(index_files)}\")\n",
    "        print(\"Indexed files:\")\n",
    "        for file in index_files:\n",
    "            print(f\"- {file}\")\n",
    "\n",
    "\n",
    "        # Set up evaluation\n",
    "        dataset = TaskDataset.from_name(TASK_DATASET_NAME, settings=settings)\n",
    "        metrics_callback = MeanMetricsCallback(eval_dataset=dataset)\n",
    "\n",
    "        evaluator = Evaluator(\n",
    "            config=EvaluatorConfig(batch_size=3),\n",
    "            agent=SimpleAgent(),\n",
    "            dataset=dataset,\n",
    "            callbacks=[metrics_callback],\n",
    "        )\n",
    "        await evaluator.evaluate()\n",
    "\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(metrics_callback.eval_means)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[bold red]Removing Deep Visual Proteomics advances human colon organoid models by revealing a switch to an in vivo-like phenotype upon xenotransplantation.pdf from index.[/bold red]\n",
      "[bold red]Files removed![/bold red]\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "Metadata not found for 10.1101/2024.04.23.590729 in SemanticScholarProvider.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "Metadata not found for 10.1073/pnas.XXXXXXXXXX in SemanticScholarProvider.\n",
      "Metadata not found for 10.1073/pnas.XXXXXXXXXX in CrossrefProvider.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "Metadata not found for 10.xxxx in CrossrefProvider.\n",
      "Metadata not found for 10.7554/eLife.89176 in CrossrefProvider.\n",
      "Metadata not found for 10.xxxx in SemanticScholarProvider.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "Metadata not found for 10.1093/nar/gkae122 in SemanticScholarProvider.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "Metadata not found for 10.1101/2024.04.29.591747 in SemanticScholarProvider.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n",
      "SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index: pqa_index_73db35b7edb188828b5799b8ac0a3fac\n",
      "Number of indexed files: 49\n",
      "Indexed files:\n",
      "- DiffDomain enables identification of structurally reorganized topologically associating domains.pdf\n",
      "- Full-length tRNAs lacking a functional CCA tail are selectively sorted into the lumen of extracellular vesicles.pdf\n",
      "- Type-I-interferon-responsive microglia shape cortical development and behavior.pdf\n",
      "- liang-et-al-2024-a-conserved-molecular-logic-for-neurogenesis-to-gliogenesis-switch-in-the-cerebral-cortex.pdf\n",
      "- goyette-et-al-2024-cancer-stromal-cell-interactions-in-breast-cancer-brain-metastases-induce-glycocalyx-mediated.pdf\n",
      "- Light regulates widespread plant alternative polyadenylation 2 through the chloroplast.pdf\n",
      "- High-speed imaging of giant unilamellar vesicle formation in cDICE.pdf\n",
      "- elife-90425-v2.pdf\n",
      "- sherman-et-al-2024-the-fatty-liver-disease-causing-protein-pnpla3-i148m-alters-lipid-droplet-golgi-dynamics.pdf\n",
      "- Role of m6A RNA methylation in dosage compensation.pdf\n",
      "- Functional analysis of the mating type genes in Verticillium dahliae.pdf\n",
      "- ulicevic-et-al-2024-uncovering-the-dynamics-and-consequences-of-rna-isoform-changes-during-neuronal-differentiation.pdf\n",
      "- An improved epigenetic counter to track mitotic age in normal and precancerous tissues.pdf\n",
      "- Probing enzyme-dependent pseudouridylation using direct RNA sequencing to assess neuronal epitranscriptome plasticity.pdf\n",
      "- Leveraging chromatin state transitions for the identification of regulatory networks orchestrating heart regeneration.pdf\n",
      "- Design of a self-regulating mRNA gene circuit.pdf\n",
      "- RNA damage compartmentalization by DHX9 stress granules.pdf\n",
      "- CD80 on skin stem cells promotes local expansion of regulatory T cells upon injury to orchestrate repair within an inflammatory environment.pdf\n",
      "- doddihal-et-al-2024-a-pak-family-kinase-and-the-hippo-yorkie-pathway-modulate-wnt-signaling-to-functionally-integrate.pdf\n",
      "- wolter-et-al-2024-diet-driven-differential-response-of-akkermansia-muciniphila-modulates-pathogen-susceptibility.pdf\n",
      "- Integrative analysis of transcriptomic and epigenomic data reveals distinct patterns for developmental and housekeeping gene regulation.pdf\n",
      "- fromm-et-al-2024-translocation-of-yopj-family-effector-proteins-through-the-virb-vird4-t4ss-of-bartonella.pdf\n",
      "- Active droplets through enzyme-free, dynamic phosphorylation.pdf\n",
      "- Migrasomal autophagosomes relieve BMC Biology Open Access endoplasmic reticulum stress in glioblastoma cells.pdf\n",
      "- Ubiquitin ligase and signalling hub MYCBP2 is required for efficient EPHB2 tyrosine kinase receptor function .pdf\n",
      "- Stiffness-dependent LOX regulation via HIF-1 drives extracellular matrix modifications in psoriasis.pdf\n",
      "- The 18S rRNA Methyltransferase DIMT-1 Regulates Lifespan in the Germline Later in Life.pdf\n",
      "- Selective haematological cancer eradication with preserved haematopoiesis.pdf\n",
      "- Visualization of intracellular ATP dynamics in the whole kidney under pathophysiological conditions using the kidney slice culture system.pdf\n",
      "- The origin recognition complex requires chromatin tethering by a hypervariable intrinsically disordered region that is functionally conserved from sponge to man.pdf\n",
      "- cisneros-et-al-2024-mutational-biases-favor-complexity-increases-in-protein-interaction-networks-after-gene-duplication.pdf\n",
      "- Human cortical neurogenesis is altered via glucocorticoid-mediated regulation of ZBTB16 expression.pdf\n",
      "- Cell-type-specific expression of tRNAs in the brain regulates cellular homeostasis.pdf\n",
      "- BRD2 promotes antibody class switch recombination by facilitating DNA repair in collaboration with NIPBL.pdf\n",
      "- A LATS2 and ALKBH5 positive feedback loop supports their oncogenic roles.pdf\n",
      "- Heat tolerance, oxidative stress response tuning, and robust gene activation in early-stage Drosophila melanogaster embryos .pdf\n",
      "- Deciphering signaling pathways in hematopoietic stem cells- the molecular complexity of Myelodysplastic Syndromes (MDS) and leukemic progression.pdf\n",
      "- Assembly of SARS-CoV-2 nucleocapsid protein with nucleic acid.pdf\n",
      "- gualdrini-et-al-2024-an-integrative-epigenome-based-strategy-for-unbiased-functional-profiling-of-clinical-kinase.pdf\n",
      "- Deep Visual Proteomics advances human colon organoid 2 models by revealing a switch to an in vivo-like phenotype upon 3 xenotransplantation.pdf\n",
      "- Liver X Receptor Ligand GAC0001E5 Downregulates Antioxidant Capacity and ERBB2:HER2 Expression in HER2-Positive Breast Cancer Cells.pdf\n",
      "- DNA methylation.pdf\n",
      "- Mouse Genome Informatics- an integrated knowledgebase system for the laboratory mouse.pdf\n",
      "- Ribosomal collision is not a prerequisite for ZNF598-mediated ribosome ubiquitination and disassembly of ribosomal complexes by ASCC.pdf\n",
      "- Embryos assist morphogenesis of others through calcium and ATP signaling mechanisms in collective teratogen resistance.pdf\n",
      "- Patterned apoptosis has an instructive role for local growth and tissue shape regulation in a fast-growing epithelium.pdf\n",
      "- Pathogenic CANVAS (AAGGG)n repeats stall DNA replication due to the formation of alternative DNA structures.pdf\n",
      "- hellsberg-et-al-2024-identification-of-the-potassium-binding-site-in-serotonin-transporter.pdf\n",
      "- geller-et-al-2024-identification-of-type-vi-secretion-system-effector-immunity-pairs-using-structural-bioinformatics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'reward': -0.20250000000000007, 'truncation_rate': 0.0, 'avg_value': 0.0, 'num_steps': 11.025, 'failures': 0.0, 'total_paper_count': 17.475, 'relevant_paper_count': 0.075, 'evidence_count': 0.1, 'correct': 0.0, 'correct_unsure': 0.725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "paper_dir = '/Users/apple/Documents/GitLab_Projects/master_project/xx823/papers'\n",
    "await amain(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    llm_model: str  # For generate answer\n",
    "    summary_llm_model: str | None = None  # For RCS, None means no RCS\n",
    "    search_count: int = 12\n",
    "    top_k: int = 30\n",
    "    agent_evidence_n: int = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_evaluation_resources(\n",
    "    paper_directory: Path,\n",
    "    cache_dir: Path | None = None,\n",
    ") -> tuple[Path, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Setup resources needed for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        paper_directory: Directory where papers will be stored\n",
    "        cache_dir: Optional cache directory for dataset downloads\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (paper directory path, train DataFrame, eval DataFrame, test DataFrame)\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    paper_directory.mkdir(parents=True, exist_ok=True)\n",
    "    if cache_dir:\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading LitQA2 datasets...\")\n",
    "    train_df, eval_df, test_df = read_litqa_v2_from_hub(\n",
    "        train_eval_dataset=DEFAULT_LABBENCH_HF_HUB_NAME,\n",
    "        test_dataset=DEFAULT_AVIARY_PAPER_HF_HUB_NAME,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    # Download papers if not already present\n",
    "    print(\"Checking/downloading required papers...\")\n",
    "    \n",
    "    # Get all unique DOIs from datasets\n",
    "    all_sources = set()\n",
    "    for df in [train_df, eval_df, test_df]:\n",
    "        for sources in df['sources']:\n",
    "            all_sources.update(sources)\n",
    "    \n",
    "    # Here you would implement paper downloading logic\n",
    "    # This is a placeholder - you'll need to implement actual paper downloading\n",
    "    # based on your institution's access and legal requirements\n",
    "    for source in all_sources:\n",
    "        target_file = paper_directory / f\"{source}.pdf\"\n",
    "        if not target_file.exists():\n",
    "            print(f\"Need to acquire paper: {source}\")\n",
    "    \n",
    "    return paper_directory, train_df, eval_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_model(\n",
    "    config: ModelConfig,\n",
    "    paper_directory: str | os.PathLike,\n",
    "    split: str = LitQAv2TaskSplit.EVAL,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Run evaluation for a specific model configuration.\"\"\"\n",
    "    \n",
    "    # Configure settings\n",
    "    agent_settings = AgentSettings(\n",
    "        search_count=config.search_count,\n",
    "        top_k=config.top_k,\n",
    "        agent_evidence_n=config.agent_evidence_n,\n",
    "        index=IndexSettings(paper_directory=paper_directory),\n",
    "    )\n",
    "    \n",
    "    settings = Settings(\n",
    "        agent=agent_settings,\n",
    "        llm_model=config.llm_model,\n",
    "        summary_llm_model=config.summary_llm_model if config.summary_llm_model else config.llm_model,\n",
    "    )\n",
    "\n",
    "    # Create dataset and evaluation setup\n",
    "    dataset = TaskDataset.from_name(\n",
    "        TASK_DATASET_NAME,\n",
    "        settings=settings,\n",
    "        split=split,\n",
    "    )\n",
    "    metrics_callback = MeanMetricsCallback(eval_dataset=dataset)\n",
    "    \n",
    "    evaluator = Evaluator(\n",
    "        config=EvaluatorConfig(batch_size=3),\n",
    "        agent=SimpleAgent(),\n",
    "        dataset=dataset,\n",
    "        callbacks=[metrics_callback],\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    await evaluator.evaluate()\n",
    "    return metrics_callback.eval_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluations(\n",
    "    paper_directory: str | os.PathLike,\n",
    "    cache_dir: Path | None = None\n",
    ") -> None:\n",
    "    \"\"\"Run evaluations for different model configurations.\"\"\"\n",
    "    \n",
    "    # Setup resources first\n",
    "    paper_dir, train_df, eval_df, test_df = await setup_evaluation_resources(\n",
    "        Path(paper_directory),\n",
    "        cache_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded dataset splits:\")\n",
    "    print(f\"Train: {len(train_df)} questions\")\n",
    "    print(f\"Eval: {len(eval_df)} questions\")\n",
    "    print(f\"Test: {len(test_df)} questions\")\n",
    "    \n",
    "    configs = [\n",
    "        # Base models without RCS\n",
    "        ModelConfig(\n",
    "            name=\"No RCS\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=None,\n",
    "        ),\n",
    "        \n",
    "        # Different models with RCS\n",
    "        ModelConfig(\n",
    "            name=\"GPT-4 Turbo\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Claude-3-Opus\",\n",
    "            llm_model=\"claude-3-opus-20240229\",\n",
    "            summary_llm_model=\"claude-3-opus-20240229\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Gemini-1.5-Pro\",\n",
    "            llm_model=\"gemini-1.5-pro\",\n",
    "            summary_llm_model=\"gemini-1.5-pro\",\n",
    "        ),\n",
    "        \n",
    "        # Ablation studies\n",
    "        ModelConfig(\n",
    "            name=\"Evidence@5\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "            agent_evidence_n=5,\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Top-k@10\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "            top_k=10,\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for config in configs:\n",
    "        print(f\"Evaluating {config.name}...\")\n",
    "        metrics = await evaluate_model(config, paper_directory, cache_dir=cache_dir)\n",
    "        results[config.name] = metrics\n",
    "        \n",
    "        # Print key metrics\n",
    "        print(f\"\\nResults for {config.name}:\")\n",
    "        print(f\"Accuracy: {metrics['correct']:.3f}\")\n",
    "        print(f\"Precision: {metrics['correct'] / (1 - metrics['unsure']):.3f}\")\n",
    "        print(f\"Average Evidence Count: {metrics['evidence_count']:.1f}\")\n",
    "        print(\"----------------------------------------\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m paper_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpapers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "paper_dir = Path(\"papers\")\n",
    "cache_dir = Path(\"cache\")\n",
    "results = asyncio.run(run_evaluations(paper_dir, cache_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Mapping\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "from aviary.env import TaskDataset\n",
    "from ldp.agent import SimpleAgent\n",
    "from ldp.alg.callbacks import MeanMetricsCallback\n",
    "from ldp.alg.runners import Evaluator, EvaluatorConfig\n",
    "from paperqa import Settings\n",
    "from paperqa.agents.task import TASK_DATASET_NAME, LitQAv2TaskSplit\n",
    "from paperqa.settings import AgentSettings, IndexSettings\n",
    "from paperqa.litqa import (\n",
    "    read_litqa_v2_from_hub,\n",
    "    DEFAULT_LABBENCH_HF_HUB_NAME,\n",
    "    DEFAULT_AVIARY_PAPER_HF_HUB_NAME,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 199/199 [00:00<00:00, 3764.44 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question:\n",
      "Question: Acinetobacter lwoffii has been evolved in the lab to be resistant to which of these antibiotics?\n",
      "Correct Answer: ciproflaxin\n",
      "Distractors: ['meropenem', 'gentamicin', 'ampicillin']\n",
      "Sources: ['https://doi.org/10.1128/msphere.00109-24']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load train/eval dataset\n",
    "train_eval_data = load_dataset(\"futurehouse/lab-bench\", \"LitQA2\")[\"train\"]\n",
    "\n",
    "# Look at first question from test set\n",
    "print(\"Sample question:\")\n",
    "print(f\"Question: {train_eval_data[0]['question']}\")\n",
    "print(f\"Correct Answer: {train_eval_data[0]['ideal']}\")\n",
    "print(f\"Distractors: {train_eval_data[0]['distractors']}\")\n",
    "print(f\"Sources: {train_eval_data[0]['sources']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    llm_model: str  # For generate answer\n",
    "    summary_llm_model: str | None = None  # For RCS, None means no RCS\n",
    "    search_count: int = 12\n",
    "    top_k: int = 30\n",
    "    agent_evidence_n: int = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_evaluation_resources(\n",
    "    paper_directory: Path,\n",
    "    cache_dir: Path | None = None,\n",
    ") -> tuple[Path, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Setup resources needed for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        paper_directory: Directory where papers will be stored\n",
    "        cache_dir: Optional cache directory for dataset downloads\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (paper directory path, train DataFrame, eval DataFrame, test DataFrame)\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    paper_directory.mkdir(parents=True, exist_ok=True)\n",
    "    if cache_dir:\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading LitQA2 datasets...\")\n",
    "    train_df, eval_df, test_df = read_litqa_v2_from_hub(\n",
    "        train_eval_dataset=DEFAULT_LABBENCH_HF_HUB_NAME,\n",
    "        test_dataset=DEFAULT_AVIARY_PAPER_HF_HUB_NAME,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    # Download papers if not already present\n",
    "    print(\"Checking/downloading required papers...\")\n",
    "    \n",
    "    # Get all unique DOIs from datasets\n",
    "    all_sources = set()\n",
    "    for df in [train_df, eval_df, test_df]:\n",
    "        for sources in df['sources']:\n",
    "            all_sources.update(sources)\n",
    "    \n",
    "    # Here you would implement paper downloading logic\n",
    "    # This is a placeholder - you'll need to implement actual paper downloading\n",
    "    # based on your institution's access and legal requirements\n",
    "    for source in all_sources:\n",
    "        target_file = paper_directory / f\"{source}.pdf\"\n",
    "        if not target_file.exists():\n",
    "            print(f\"Need to acquire paper: {source}\")\n",
    "    \n",
    "    return paper_directory, train_df, eval_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_model(\n",
    "    config: ModelConfig,\n",
    "    paper_directory: str | os.PathLike,\n",
    "    split: str = LitQAv2TaskSplit.EVAL,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Run evaluation for a specific model configuration.\"\"\"\n",
    "    \n",
    "    # Configure settings\n",
    "    agent_settings = AgentSettings(\n",
    "        search_count=config.search_count,\n",
    "        top_k=config.top_k,\n",
    "        agent_evidence_n=config.agent_evidence_n,\n",
    "        index=IndexSettings(paper_directory=paper_directory),\n",
    "    )\n",
    "    \n",
    "    settings = Settings(\n",
    "        agent=agent_settings,\n",
    "        llm_model=config.llm_model,\n",
    "        summary_llm_model=config.summary_llm_model if config.summary_llm_model else config.llm_model,\n",
    "    )\n",
    "\n",
    "    # Create dataset and evaluation setup\n",
    "    dataset = TaskDataset.from_name(\n",
    "        TASK_DATASET_NAME,\n",
    "        settings=settings,\n",
    "        split=split,\n",
    "    )\n",
    "    metrics_callback = MeanMetricsCallback(eval_dataset=dataset)\n",
    "    \n",
    "    evaluator = Evaluator(\n",
    "        config=EvaluatorConfig(batch_size=3),\n",
    "        agent=SimpleAgent(),\n",
    "        dataset=dataset,\n",
    "        callbacks=[metrics_callback],\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    await evaluator.evaluate()\n",
    "    return metrics_callback.eval_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluations(\n",
    "    paper_directory: str | os.PathLike,\n",
    "    cache_dir: Path | None = None\n",
    ") -> None:\n",
    "    \"\"\"Run evaluations for different model configurations.\"\"\"\n",
    "    \n",
    "    # Setup resources first\n",
    "    paper_dir, train_df, eval_df, test_df = await setup_evaluation_resources(\n",
    "        Path(paper_directory),\n",
    "        cache_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded dataset splits:\")\n",
    "    print(f\"Train: {len(train_df)} questions\")\n",
    "    print(f\"Eval: {len(eval_df)} questions\")\n",
    "    print(f\"Test: {len(test_df)} questions\")\n",
    "    \n",
    "    configs = [\n",
    "        # Base models without RCS\n",
    "        ModelConfig(\n",
    "            name=\"No RCS\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=None,\n",
    "        ),\n",
    "        \n",
    "        # Different models with RCS\n",
    "        ModelConfig(\n",
    "            name=\"GPT-4 Turbo\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Claude-3-Opus\",\n",
    "            llm_model=\"claude-3-opus-20240229\",\n",
    "            summary_llm_model=\"claude-3-opus-20240229\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Gemini-1.5-Pro\",\n",
    "            llm_model=\"gemini-1.5-pro\",\n",
    "            summary_llm_model=\"gemini-1.5-pro\",\n",
    "        ),\n",
    "        \n",
    "        # Ablation studies\n",
    "        ModelConfig(\n",
    "            name=\"Evidence@5\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "            agent_evidence_n=5,\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            name=\"Top-k@10\",\n",
    "            llm_model=\"gpt-4-0125-preview\",\n",
    "            summary_llm_model=\"gpt-4-0125-preview\",\n",
    "            top_k=10,\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for config in configs:\n",
    "        print(f\"Evaluating {config.name}...\")\n",
    "        metrics = await evaluate_model(config, paper_directory, cache_dir=cache_dir)\n",
    "        results[config.name] = metrics\n",
    "        \n",
    "        # Print key metrics\n",
    "        print(f\"\\nResults for {config.name}:\")\n",
    "        print(f\"Accuracy: {metrics['correct']:.3f}\")\n",
    "        print(f\"Precision: {metrics['correct'] / (1 - metrics['unsure']):.3f}\")\n",
    "        print(f\"Average Evidence Count: {metrics['evidence_count']:.1f}\")\n",
    "        print(\"----------------------------------------\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m paper_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpapers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/paperqa_env/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "paper_dir = Path(\"papers\")\n",
    "cache_dir = Path(\"cache\")\n",
    "results = asyncio.run(run_evaluations(paper_dir, cache_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
